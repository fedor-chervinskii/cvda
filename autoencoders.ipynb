{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Training an Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will train an autoencoder to model images of faces. For this we take \"Labeled Faces in the Wild\" dataset (LFW) (http://vis-www.cs.umass.edu/lfw/), deep funneled version of it. (frontal view of all faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collab setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you're running in colab,\n",
    "# 1. go to Runtime -> Change Runtimy Type -> GPU\n",
    "# 2. uncomment this:\n",
    "#!wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/hw3_19/homework03/lfw_dataset.py -O lfw_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line fetches you two datasets: images, usable for autoencoder training and attributes.\n",
    "# Those attributes will be required for the final part of the assignment (applying smiles), so please keep them in mind\n",
    "from lfw_dataset import fetch_lfw_dataset\n",
    "data,attrs = fetch_lfw_dataset(dimx=36,dimy=36)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data/255\n",
    "np.savez(\"real.npz\", Pictures=data.reshape(data.shape[0], 36*36*3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = data[:10000].reshape((10000, -1))\n",
    "print(X_train.shape)\n",
    "X_val = data[10000:].reshape((-1, X_train.shape[1]))\n",
    "print(X_val.shape)\n",
    "\n",
    "image_h = data.shape[1]\n",
    "image_w = data.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we want all values of the data to lie in the interval $[0,1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.float32(X_train)\n",
    "X_val = np.float32(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gallery(images, h, w, n_row=3, n_col=6):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.5 * n_col, 1.7 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w, 3)), cmap=plt.cm.gray, vmin=-1, vmax=1, interpolation='nearest')\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gallery(X_train, image_h, image_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_utils.TensorDataset(torch.Tensor(X_train), torch.zeros(X_train.shape[0],)) # pseudo labels needed to define TensorDataset\n",
    "train_loader = data_utils.DataLoader(train, batch_size=100, shuffle=True)\n",
    "\n",
    "val = data_utils.TensorDataset(torch.Tensor(X_val), torch.zeros(X_val.shape[0],))\n",
    "val_loader = data_utils.DataLoader(val, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "Why to use all this complicated formulaes and regularizations, what is the need for variational inference? To analyze the difference, let's first train just an autoencoder on the data:\n",
    "\n",
    "<img src=\"Autoencoder_structure.png\" alt=\"Autoencoder\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimZ = 100 # Considering face reconstruction task, which size of representation seems reasonable?\n",
    "\n",
    "# Define the decoder and encoder as networks with one hidden fc-layer\n",
    "# (that means you will have 2 fc layers in each net)\n",
    "# Use ReLU for hidden layers' activations\n",
    "# GlorotUniform initialization for W\n",
    "# Zero initialization for biases\n",
    "# It's also convenient to put sigmoid activation on output layer to get nice normalized pics\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        #TODO\n",
    "        \n",
    "        # self.encoder = \n",
    "        # self.decoder =\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #TODO\n",
    "        \n",
    "        # latent_code = \n",
    "        # reconstruction = \n",
    "        \n",
    "        return reconstruction, latent_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MSE loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "autoencoder = Autoencoder()#.cuda()\n",
    "\n",
    "# Use Adam optimizer\n",
    "optimizer = optim.Adam(autoencoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train your autoencoder\n",
    "autoencoder.train()\n",
    "for epoch in range(100):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "    #    data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, _ = autoencoder(data)\n",
    "        loss = loss_function(recon_batch, data)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examine the reconstructions\n",
    "for j, data in enumerate(val_loader, 0):\n",
    "    inp = Variable(data[0].cuda())\n",
    "    pred, _ = autoencoder(inp)\n",
    "    plot_gallery([data[0].numpy(), pred.data.cpu().numpy()], image_h, image_w, n_row=1, n_col=2)\n",
    "    if (j >= 9):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruction is not bad, right? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (putin, y) in enumerate(val_loader):\n",
    "    if i == 2754:\n",
    "        break\n",
    "plt.imshow(putin.numpy().reshape((image_w, image_w, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "plt.suptitle('Twin farm')\n",
    "for i in range(len(image_progress[:20])):\n",
    "    plt.subplots_adjust(bottom=0.0, left=.1, right=.9, top=.50, hspace=.15)\n",
    "    plt.subplot(6, 5, 5*(i//5) + i % 5 + 1)\n",
    "    plt.imshow(image_progress[i].clamp(0,1).data.cpu().numpy().reshape(image_w, image_h, 3))\n",
    "    plt.title('Epoch = {}'.format(i * 5 + 1))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now sample several latent vectors and perform inference from $z$, reconstruct an image given some random $z$ representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = (np.random.randn(25, dimZ)*0.5).astype('float32')\n",
    "output = autoencoder.decoder(Variable(torch.from_numpy(z)).cuda()).clamp(0, 1)\n",
    "plot_gallery(output.data.cpu().numpy(), image_h, image_w, n_row=5, n_col=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we sample $z$ from normal, whould we eventually generate all possible faces? What do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Congrats! and Bonus\n",
    "\n",
    "If you managed to tune your autoencoders to converge and learn something about the world, now it's time to make fun out of it. As you may have noticed, there are face attributes in dataset. We're interesting in \"Smiling\" column, but feel free to try others as well! Here is the first task:\n",
    "\n",
    "1) Extract the \"Smilling\" attribute and create a two sets of images: 10 smiling faces and 10 non-smiling ones.\n",
    "\n",
    "2) Compute latent representations for each image in \"smiling\" set and average those latent vectors. Do the same for \"non-smiling\" set. You have found **\"vector representation\"** of the \"smile\" and \"no smile\" attribute.\n",
    "\n",
    "3) Compute the difference: \"smile\" vector minus \"non-smile\" vector.\n",
    "\n",
    "3) Now check if **\"feature arithmetics\"** works. Sample a face without smile, encode it and add the diff from p. 3. Check if it works with both AE and VAE. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
